# Hugging Face - Curso Completo ğŸš€

Este repositÃ³rio contÃ©m os materiais e exemplos prÃ¡ticos do curso sobre **Hugging Face**, abordando desde a exploraÃ§Ã£o de modelos prÃ©-treinados atÃ© a escalabilidade no treinamento de modelos.

## ğŸ“Œ ConteÃºdo do Curso

### 1ï¸âƒ£ Explorando Modelos PrÃ©-Treinados
Neste passo inicial, vocÃª serÃ¡ introduzido ao Hugging Face, uma das principais plataformas de inteligÃªncia artificial, focada em modelos de processamento de linguagem natural. 

- IntroduÃ§Ã£o ao Hugging Face
- UtilizaÃ§Ã£o de modelos prÃ©-treinados para NLP e visÃ£o computacional
- Hands-on com pipelines e inferÃªncia

### 2ï¸âƒ£ Otimizando Modelos PrÃ©-Treinados
Aqui vocÃª aprenderÃ¡ tÃ©cnicas avanÃ§adas para melhorar modelos de linguagem de larga escala.

- Transfer learning e fine-tuning
- Uso do Optimum para otimizaÃ§Ã£o de modelos
- CustomizaÃ§Ã£o de modelos para tarefas especÃ­ficas

### 3ï¸âƒ£ Escalando o Treinamento de Modelos
No passo final, exploramos tÃ©cnicas para treinar modelos de forma eficiente em grandes infraestruturas.

- IntroduÃ§Ã£o ao Hugging Face Accelerate
- Treinamento distribuÃ­do em diferentes configuraÃ§Ãµes de hardware
- EstratÃ©gias para escalar modelos de NLP

---

## ğŸ“¦ InstalaÃ§Ã£o

Antes de comeÃ§ar, certifique-se de ter o Python instalado. VocÃª pode instalar as bibliotecas necessÃ¡rias com:

```bash
pip install transformers torch datasets accelerate optimum
```

Se precisar de aceleraÃ§Ã£o por GPU, instale o **CUDA** para PyTorch seguindo as instruÃ§Ãµes no site oficial do PyTorch.

---

## ğŸš€ Exemplos de CÃ³digo

### ğŸ”¹ Explorando Modelos PrÃ©-Treinados

```python
from transformers import pipeline

# Criando um pipeline de anÃ¡lise de sentimento
sentiment_pipeline = pipeline("sentiment-analysis")

# Testando o modelo
result = sentiment_pipeline("Hugging Face Ã© incrÃ­vel!")
print(result)
```

### ğŸ”¹ Otimizando Modelos PrÃ©-Treinados

```python
from transformers import AutoModelForSequenceClassification, AutoTokenizer
import torch

model_name = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

# Tokenizando um texto
inputs = tokenizer("Hugging Face facilita o NLP!", return_tensors="pt")
outputs = model(**inputs)

# Obtendo previsÃµes
logits = outputs.logits
predicted_class = torch.argmax(logits).item()
print(f"Classe prevista: {predicted_class}")
```

### ğŸ”¹ Escalando o Treinamento de Modelos

```python
from transformers import Trainer, TrainingArguments
from datasets import load_dataset
from accelerate import Accelerator

# ConfiguraÃ§Ã£o de aceleraÃ§Ã£o
accelerator = Accelerator()

# Carregar dataset
raw_datasets = load_dataset("imdb")

# Definir argumentos de treinamento
training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=raw_datasets["train"],
    eval_dataset=raw_datasets["test"]
)

# Treinar o modelo com aceleraÃ§Ã£o
with accelerator:
    trainer.train()
```

---

## ğŸ“– DocumentaÃ§Ã£o Oficial

Para mais detalhes sobre a biblioteca **Transformers**, consulte a documentaÃ§Ã£o oficial: [Hugging Face Docs](https://huggingface.co/docs/transformers/index)

## ğŸ“œ LicenÃ§a

Este projeto segue a licenÃ§a MIT. Sinta-se Ã  vontade para contribuir! ğŸš€
